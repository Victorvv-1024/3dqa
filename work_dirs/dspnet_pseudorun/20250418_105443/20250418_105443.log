2025/04/18 10:54:43 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 42
    GPU 0: NVIDIA GeForce RTX 4080 SUPER
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.8, V11.8.89
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.2.0
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.2 (Git Hash 2dc95a2ad0841e29db8b22fbccaf3e5da7992b01)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.2.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.17.0
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 42
    diff_rank_seed: True
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/04/18 10:54:43 - mmengine - INFO - Config:
backbone_lidar_inchannels = 6
backend_args = None
classes = (
    'cabinet',
    'bed',
    'chair',
    'sofa',
    'table',
    'door',
    'window',
    'bookshelf',
    'picture',
    'counter',
    'desk',
    'curtain',
    'refrigerator',
    'shower curtain',
    'toilet',
    'sink',
    'bathtub',
    'others',
)
compile_options = dict(backend='inductor', mode='max-autotune')
custom_hooks = [
    dict(after_iter=True, type='EmptyCacheHook'),
]
data_root = 'data'
dataset_type = 'MultiViewScanQADataset'
default_hooks = dict(
    checkpoint=dict(
        interval=1,
        max_keep_ckpts=3,
        rule='greater',
        save_best='EM@1',
        type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'))
default_scope = 'embodiedqa'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
find_unused_parameters = True
launcher = 'none'
load_from = './work_dirs/scannet-det/scannet-votenet-12xb12/epoch_12.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
lr = 0.0001
max_epochs = 12
model = dict(
    backbone=dict(
        add_map=True,
        frozen=True,
        name='microsoft/swin-base-patch4-window7-224-in22k',
        out_channels=[
            1024,
        ],
        type='SwinModelWrapper'),
    backbone_fusion=dict(
        hidden_size=768,
        num_attention_heads=12,
        num_hidden_layers=4,
        type='CrossModalityEncoder'),
    backbone_lidar=dict(
        fp_channels=(
            (
                256,
                256,
            ),
            (
                256,
                256,
            ),
        ),
        frozen=False,
        in_channels=6,
        norm_cfg=dict(type='BN2d'),
        num_points=(
            2048,
            1024,
            512,
            256,
        ),
        num_samples=(
            64,
            32,
            16,
            16,
        ),
        radius=(
            0.2,
            0.4,
            0.8,
            1.2,
        ),
        sa_cfg=dict(
            normalize_xyz=True,
            pool_mod='max',
            type='PointSAModule',
            use_xyz=True),
        sa_channels=(
            (
                64,
                64,
                128,
            ),
            (
                128,
                128,
                256,
            ),
            (
                128,
                128,
                256,
            ),
            (
                128,
                128,
                256,
            ),
        ),
        type='PointNet2SASSG'),
    backbone_text=dict(
        frozen=False,
        name='sentence-transformers/all-mpnet-base-v2',
        type='TextModelWrapper'),
    coord_type='DEPTH',
    data_preprocessor=dict(
        bgr_to_rgb=True,
        furthest_point_sample=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        num_points=40000,
        pad_size_divisor=32,
        std=[
            58.394,
            57.12,
            57.375,
        ],
        type='Det3DDataPreprocessor',
        use_imagenet_default_mean_std=True),
    qa_head=dict(
        dropout=0.3,
        hidden_channels=768,
        in_channels=768,
        num_classes=8864,
        type='QAHead'),
    target_bbox_head=dict(
        bbox_coder=dict(
            mean_sizes=[
                [
                    0.775,
                    0.949,
                    0.9654,
                ],
                [
                    1.869,
                    1.8321,
                    1.1922,
                ],
                [
                    0.6121,
                    0.6193,
                    0.7048,
                ],
                [
                    1.4411,
                    1.6045,
                    0.8365,
                ],
                [
                    1.0478,
                    1.2016,
                    0.6346,
                ],
                [
                    0.561,
                    0.6085,
                    1.7195,
                ],
                [
                    1.0789,
                    0.8203,
                    1.1692,
                ],
                [
                    0.8417,
                    1.3505,
                    1.6899,
                ],
                [
                    0.2305,
                    0.4764,
                    0.5657,
                ],
                [
                    1.4548,
                    1.9712,
                    0.2864,
                ],
                [
                    1.0786,
                    1.5371,
                    0.865,
                ],
                [
                    1.4312,
                    0.7692,
                    1.6498,
                ],
                [
                    0.6297,
                    0.7087,
                    1.3143,
                ],
                [
                    0.4393,
                    0.4157,
                    1.7,
                ],
                [
                    0.585,
                    0.5788,
                    0.7203,
                ],
                [
                    0.5116,
                    0.5096,
                    0.3129,
                ],
                [
                    1.1732,
                    1.0599,
                    0.5181,
                ],
                [
                    0.4329,
                    0.5193,
                    0.4844,
                ],
            ],
            num_dir_bins=10,
            num_sizes=18,
            type='PartialBinBasedBBoxCoder',
            with_rot=True),
        dropout=0.3,
        hidden_channels=768,
        in_channels=768,
        loss_weight=1.0,
        num_classes=1,
        train_cfg=dict(neg_distance_thr=0.6, pos_distance_thr=0.3),
        type='RefLocHead'),
    target_cls_head=dict(
        dropout=0.3,
        hidden_channels=768,
        in_channels=1536,
        loss_weight=1.0,
        num_classes=18,
        type='RefClsHead'),
    test_cfg=dict(
        nms_thr=0.25,
        per_class_proposal=True,
        sample_mode='seed',
        score_thr=0.05),
    text_max_length=512,
    train_cfg=dict(
        neg_distance_thr=0.6, pos_distance_thr=0.3, sample_mode='seed'),
    type='MultiViewVLMBase3DQA',
    voxel_size=0.01)
n_points = 40000
optim_wrapper = dict(
    accumulative_counts=1,
    clip_grad=dict(max_norm=10, norm_type=2),
    optimizer=dict(lr=0.0001, type='AdamW', weight_decay=1e-05),
    paramwise_cfg=dict(
        bypass_duplicate=True,
        custom_keys=dict(text_encoder=dict(lr_mult=0.1))),
    type='OptimWrapper')
param_scheduler = [
    dict(
        T_max=12,
        begin=0,
        by_epoch=True,
        convert_to_iter_based=True,
        end=12,
        eta_min_ratio=0.05,
        type='CosineAnnealingLR'),
    dict(begin=0, by_epoch=False, end=500, start_factor=0.05, type='LinearLR'),
]
randomness = dict(diff_rank_seed=True, seed=42)
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=12,
    dataset=dict(
        ann_file='mv_scannetv2_infos_val.pkl',
        anno_indices=None,
        box_type_3d='Depth',
        data_root='data',
        filter_empty_gt=False,
        metainfo=dict(
            classes=(
                'cabinet',
                'bed',
                'chair',
                'sofa',
                'table',
                'door',
                'window',
                'bookshelf',
                'picture',
                'counter',
                'desk',
                'curtain',
                'refrigerator',
                'shower curtain',
                'toilet',
                'sink',
                'bathtub',
                'others',
            )),
        pipeline=[
            dict(type='LoadAnnotations3D', with_answer_labels=True),
            dict(
                n_images=20,
                ordered=True,
                transforms=[
                    dict(backend_args=None, type='LoadImageFromFile'),
                    dict(backend_args=None, type='LoadDepthFromFile'),
                    dict(
                        coord_type='CAMERA',
                        type='ConvertRGBDToPoints',
                        use_color=0),
                    dict(num_points=4000, type='PointSample'),
                    dict(keep_ratio=False, scale=(
                        224,
                        224,
                    ), type='Resize'),
                ],
                type='MultiViewPipeline'),
            dict(
                coord_type='DEPTH',
                save_views_points=True,
                type='AggregateMultiViewPoints',
                use_clean_global_points=True,
                use_color=True),
            dict(
                keys=[
                    'img',
                    'points',
                    'gt_bboxes_3d',
                    'gt_labels_3d',
                    'gt_answer_labels',
                ],
                type='Pack3DDetInputs'),
        ],
        qa_file='qa/ScanQA_v1.0_test_w_obj.json',
        remove_dontcare=False,
        test_mode=True,
        type='MultiViewScanQADataset'),
    drop_last=False,
    num_workers=12,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(round_up=False, shuffle=False, type='DefaultSampler'))
test_evaluator = dict(format_only=True, type='ScanQAMetric')
test_pipeline = [
    dict(type='LoadAnnotations3D', with_answer_labels=True),
    dict(
        n_images=20,
        ordered=True,
        transforms=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(backend_args=None, type='LoadDepthFromFile'),
            dict(coord_type='CAMERA', type='ConvertRGBDToPoints', use_color=0),
            dict(num_points=4000, type='PointSample'),
            dict(keep_ratio=False, scale=(
                224,
                224,
            ), type='Resize'),
        ],
        type='MultiViewPipeline'),
    dict(
        coord_type='DEPTH',
        save_views_points=True,
        type='AggregateMultiViewPoints',
        use_clean_global_points=True,
        use_color=True),
    dict(
        keys=[
            'img',
            'points',
            'gt_bboxes_3d',
            'gt_labels_3d',
            'gt_answer_labels',
        ],
        type='Pack3DDetInputs'),
]
train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_size=12,
    dataset=dict(
        dataset=dict(
            ann_file='mv_scannetv2_infos_train.pkl',
            anno_indices=None,
            box_type_3d='Depth',
            data_root='data',
            filter_empty_gt=True,
            metainfo=dict(
                classes=(
                    'cabinet',
                    'bed',
                    'chair',
                    'sofa',
                    'table',
                    'door',
                    'window',
                    'bookshelf',
                    'picture',
                    'counter',
                    'desk',
                    'curtain',
                    'refrigerator',
                    'shower curtain',
                    'toilet',
                    'sink',
                    'bathtub',
                    'others',
                )),
            pipeline=[
                dict(
                    type='LoadAnnotations3D',
                    with_answer_labels=True,
                    with_target_objects_mask=True),
                dict(
                    n_images=20,
                    transforms=[
                        dict(backend_args=None, type='LoadImageFromFile'),
                        dict(backend_args=None, type='LoadDepthFromFile'),
                        dict(
                            coord_type='CAMERA',
                            type='ConvertRGBDToPoints',
                            use_color=0),
                        dict(num_points=4000, type='PointSample'),
                        dict(
                            keep_ratio=False,
                            scale=(
                                224,
                                224,
                            ),
                            type='Resize'),
                    ],
                    type='MultiViewPipeline'),
                dict(
                    coord_type='DEPTH',
                    save_views_points=True,
                    type='AggregateMultiViewPoints',
                    use_clean_global_points=True,
                    use_color=True),
                dict(num_points=40000, type='PointSample'),
                dict(
                    rot_range=[
                        -0.087266,
                        0.087266,
                    ],
                    scale_ratio_range=[
                        0.9,
                        1.1,
                    ],
                    shift_height=False,
                    translation_std=[
                        0.1,
                        0.1,
                        0.1,
                    ],
                    type='GlobalRotScaleTrans'),
                dict(
                    keys=[
                        'img',
                        'points',
                        'gt_bboxes_3d',
                        'gt_labels_3d',
                        'gt_answer_labels',
                        'target_objects_mask',
                    ],
                    type='Pack3DDetInputs'),
            ],
            qa_file='qa/ScanQA_v1.0_train.json',
            remove_dontcare=True,
            test_mode=False,
            type='MultiViewScanQADataset'),
        times=1,
        type='RepeatDataset'),
    drop_last=True,
    num_workers=12,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(
        type='LoadAnnotations3D',
        with_answer_labels=True,
        with_target_objects_mask=True),
    dict(
        n_images=20,
        transforms=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(backend_args=None, type='LoadDepthFromFile'),
            dict(coord_type='CAMERA', type='ConvertRGBDToPoints', use_color=0),
            dict(num_points=4000, type='PointSample'),
            dict(keep_ratio=False, scale=(
                224,
                224,
            ), type='Resize'),
        ],
        type='MultiViewPipeline'),
    dict(
        coord_type='DEPTH',
        save_views_points=True,
        type='AggregateMultiViewPoints',
        use_clean_global_points=True,
        use_color=True),
    dict(num_points=40000, type='PointSample'),
    dict(
        rot_range=[
            -0.087266,
            0.087266,
        ],
        scale_ratio_range=[
            0.9,
            1.1,
        ],
        shift_height=False,
        translation_std=[
            0.1,
            0.1,
            0.1,
        ],
        type='GlobalRotScaleTrans'),
    dict(
        keys=[
            'img',
            'points',
            'gt_bboxes_3d',
            'gt_labels_3d',
            'gt_answer_labels',
            'target_objects_mask',
        ],
        type='Pack3DDetInputs'),
]
use_clean_global_points = True
use_color = True
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=12,
    dataset=dict(
        ann_file='mv_scannetv2_infos_val.pkl',
        anno_indices=None,
        box_type_3d='Depth',
        data_root='data',
        filter_empty_gt=True,
        metainfo=dict(
            classes=(
                'cabinet',
                'bed',
                'chair',
                'sofa',
                'table',
                'door',
                'window',
                'bookshelf',
                'picture',
                'counter',
                'desk',
                'curtain',
                'refrigerator',
                'shower curtain',
                'toilet',
                'sink',
                'bathtub',
                'others',
            )),
        pipeline=[
            dict(type='LoadAnnotations3D', with_answer_labels=True),
            dict(
                n_images=20,
                ordered=True,
                transforms=[
                    dict(backend_args=None, type='LoadImageFromFile'),
                    dict(backend_args=None, type='LoadDepthFromFile'),
                    dict(
                        coord_type='CAMERA',
                        type='ConvertRGBDToPoints',
                        use_color=0),
                    dict(num_points=4000, type='PointSample'),
                    dict(keep_ratio=False, scale=(
                        224,
                        224,
                    ), type='Resize'),
                ],
                type='MultiViewPipeline'),
            dict(
                coord_type='DEPTH',
                save_views_points=True,
                type='AggregateMultiViewPoints',
                use_clean_global_points=True,
                use_color=True),
            dict(
                keys=[
                    'img',
                    'points',
                    'gt_bboxes_3d',
                    'gt_labels_3d',
                    'gt_answer_labels',
                ],
                type='Pack3DDetInputs'),
        ],
        qa_file='qa/ScanQA_v1.0_val.json',
        remove_dontcare=True,
        test_mode=True,
        type='MultiViewScanQADataset'),
    drop_last=False,
    num_workers=12,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(round_up=False, shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    result_dir='work_dirs/dspnet_pseudorun', type='ScanQAMetric')
voxel_size = 0.01
work_dir = 'work_dirs/dspnet_pseudorun'

2025/04/18 10:54:43 - mmengine - WARNING - Failed to search registry with scope "embodiedqa" in the "vis_backend" registry tree. As a workaround, the current "vis_backend" registry in "mmengine" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "embodiedqa" is a correct scope, or whether the registry is initialized.
2025/04/18 10:54:47 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/04/18 10:54:47 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
(NORMAL      ) EmptyCacheHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EmptyCacheHook                     
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EmptyCacheHook                     
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EmptyCacheHook                     
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EmptyCacheHook                     
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EmptyCacheHook                     
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EmptyCacheHook                     
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EmptyCacheHook                     
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EmptyCacheHook                     
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/04/18 10:54:47 - mmengine - WARNING - Failed to search registry with scope "embodiedqa" in the "loop" registry tree. As a workaround, the current "loop" registry in "mmengine" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "embodiedqa" is a correct scope, or whether the registry is initialized.
2025/04/18 10:54:51 - mmengine - WARNING - Failed to search registry with scope "embodiedqa" in the "data sampler" registry tree. As a workaround, the current "data sampler" registry in "mmengine" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "embodiedqa" is a correct scope, or whether the registry is initialized.
2025/04/18 10:54:51 - mmengine - WARNING - Failed to search registry with scope "embodiedqa" in the "optimizer wrapper constructor" registry tree. As a workaround, the current "optimizer wrapper constructor" registry in "mmengine" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "embodiedqa" is a correct scope, or whether the registry is initialized.
2025/04/18 10:54:51 - mmengine - WARNING - Failed to search registry with scope "embodiedqa" in the "optimizer" registry tree. As a workaround, the current "optimizer" registry in "mmengine" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "embodiedqa" is a correct scope, or whether the registry is initialized.
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.word_embeddings.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.word_embeddings.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.word_embeddings.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.position_embeddings.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.position_embeddings.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.position_embeddings.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.embeddings.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.0.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.1.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.2.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.3.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.4.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.5.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.6.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.7.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.8.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.9.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.10.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.q.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.q.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.q.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.q.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.q.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.q.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.k.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.k.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.k.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.k.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.k.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.k.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.v.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.v.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.v.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.v.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.v.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.v.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.o.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.o.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.o.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.o.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.o.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.attn.o.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.attention.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.intermediate.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.intermediate.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.intermediate.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.intermediate.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.intermediate.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.intermediate.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.LayerNorm.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.LayerNorm.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.LayerNorm.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.LayerNorm.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.LayerNorm.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.layer.11.output.LayerNorm.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.relative_attention_bias.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.relative_attention_bias.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.encoder.relative_attention_bias.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.pooler.dense.weight:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.pooler.dense.weight:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.pooler.dense.weight:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.pooler.dense.bias:lr=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.pooler.dense.bias:weight_decay=1e-05
2025/04/18 10:54:51 - mmengine - INFO - paramwise_options -- text_encoder.text_model.pooler.dense.bias:lr_mult=0.1
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.embeddings.patch_embeddings.projection.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.embeddings.patch_embeddings.projection.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.embeddings.norm.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.embeddings.norm.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.0.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.blocks.1.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.downsample.reduction.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.downsample.norm.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.0.downsample.norm.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.0.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.blocks.1.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.downsample.reduction.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.downsample.norm.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.1.downsample.norm.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.0.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.1.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.2.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.3.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.4.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.5.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.6.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.7.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.8.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.9.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.10.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.11.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.12.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.13.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.14.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.15.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.16.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.blocks.17.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.downsample.reduction.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.downsample.norm.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.2.downsample.norm.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.0.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.layernorm_before.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.layernorm_before.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.self.query.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.self.query.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.self.key.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.self.key.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.self.value.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.self.value.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.attention.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.layernorm_after.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.layernorm_after.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.intermediate.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.intermediate.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.output.dense.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.encoder.layers.3.blocks.1.output.dense.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.layernorm.weight is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - backbone.vision_model.layernorm.bias is skipped since its requires_grad=False
2025/04/18 10:54:51 - mmengine - WARNING - Failed to search registry with scope "embodiedqa" in the "optim_wrapper" registry tree. As a workaround, the current "optim_wrapper" registry in "mmengine" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "embodiedqa" is a correct scope, or whether the registry is initialized.
2025/04/18 10:54:51 - mmengine - WARNING - Failed to search registry with scope "embodiedqa" in the "parameter scheduler" registry tree. As a workaround, the current "parameter scheduler" registry in "mmengine" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "embodiedqa" is a correct scope, or whether the registry is initialized.
2025/04/18 10:54:53 - mmengine - WARNING - The prefix is not set in metric class ScanQAMetric.
Name of parameter - Initialization information

backbone_lidar.SA_modules.0.mlps.0.layer0.conv.weight - torch.Size([64, 6, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.0.mlps.0.layer0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.0.mlps.0.layer0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.0.mlps.0.layer1.conv.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.0.mlps.0.layer1.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.0.mlps.0.layer1.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.0.mlps.0.layer2.conv.weight - torch.Size([128, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.0.mlps.0.layer2.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.0.mlps.0.layer2.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer0.conv.weight - torch.Size([128, 131, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer0.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer0.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer1.conv.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer1.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer1.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer2.conv.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.1.mlps.0.layer2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer0.conv.weight - torch.Size([128, 259, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer0.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer0.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer1.conv.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer1.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer1.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer2.conv.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.2.mlps.0.layer2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer0.conv.weight - torch.Size([128, 259, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer0.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer0.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer1.conv.weight - torch.Size([128, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer1.bn.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer1.bn.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer2.conv.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.SA_modules.3.mlps.0.layer2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.0.mlps.layer0.conv.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.0.mlps.layer0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.0.mlps.layer0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.0.mlps.layer1.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.0.mlps.layer1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.0.mlps.layer1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.1.mlps.layer0.conv.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.1.mlps.layer0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.1.mlps.layer0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.1.mlps.layer1.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.1.mlps.layer1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone_lidar.FP_modules.1.mlps.layer1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.embeddings.word_embeddings.weight - torch.Size([30527, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.embeddings.position_embeddings.weight - torch.Size([514, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.embeddings.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.embeddings.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.0.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.1.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.2.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.3.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.4.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.5.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.6.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.7.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.8.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.9.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.10.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.attn.q.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.attn.q.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.attn.k.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.attn.k.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.attn.v.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.attn.v.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.attn.o.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.attn.o.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.attention.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.layer.11.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.encoder.relative_attention_bias.weight - torch.Size([32, 12]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.pooler.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_encoder.text_model.pooler.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.cross_attention.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.cross_attention.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.cross_attention.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.cross_attention.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.cross_attention_norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.cross_attention_norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.attentions.0.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.attentions.0.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.attentions.0.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.attentions.0.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.ffns.0.layers.0.0.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.ffns.0.layers.0.0.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.ffns.0.layers.1.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.ffns.0.layers.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.norms.0.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.norms.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.norms.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.0.transformer_layer.norms.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.cross_attention.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.cross_attention.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.cross_attention.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.cross_attention.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.cross_attention_norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.cross_attention_norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.attentions.0.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.attentions.0.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.attentions.0.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.attentions.0.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.ffns.0.layers.0.0.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.ffns.0.layers.0.0.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.ffns.0.layers.1.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.ffns.0.layers.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.norms.0.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.norms.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.norms.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.1.transformer_layer.norms.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.cross_attention.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.cross_attention.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.cross_attention.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.cross_attention.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.cross_attention_norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.cross_attention_norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.attentions.0.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.attentions.0.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.attentions.0.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.attentions.0.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.ffns.0.layers.0.0.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.ffns.0.layers.0.0.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.ffns.0.layers.1.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.ffns.0.layers.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.norms.0.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.norms.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.norms.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.2.transformer_layer.norms.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.cross_attention.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.cross_attention.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.cross_attention.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.cross_attention.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.cross_attention_norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.cross_attention_norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.attentions.0.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.attentions.0.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.attentions.0.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.attentions.0.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.ffns.0.layers.0.0.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.ffns.0.layers.0.0.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.ffns.0.layers.1.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.ffns.0.layers.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.norms.0.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.norms.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.norms.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder.fusion_blocks.3.transformer_layer.norms.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.embeddings.patch_embeddings.projection.weight - torch.Size([128, 3, 4, 4]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.embeddings.patch_embeddings.projection.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.embeddings.norm.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.embeddings.norm.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.layernorm_before.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.layernorm_before.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table - torch.Size([169, 4]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.self.query.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.self.query.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.self.key.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.self.key.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.self.value.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.self.value.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.output.dense.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.attention.output.dense.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.layernorm_after.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.layernorm_after.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.intermediate.dense.weight - torch.Size([512, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.intermediate.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.output.dense.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.0.output.dense.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.layernorm_before.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.layernorm_before.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table - torch.Size([169, 4]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.self.query.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.self.query.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.self.key.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.self.key.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.self.value.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.self.value.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.output.dense.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.attention.output.dense.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.layernorm_after.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.layernorm_after.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.intermediate.dense.weight - torch.Size([512, 128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.intermediate.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.output.dense.weight - torch.Size([128, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.blocks.1.output.dense.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.downsample.reduction.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.downsample.norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.0.downsample.norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.layernorm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.layernorm_before.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table - torch.Size([169, 8]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.self.query.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.self.query.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.self.key.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.self.key.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.self.value.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.self.value.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.output.dense.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.attention.output.dense.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.layernorm_after.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.layernorm_after.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.intermediate.dense.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.intermediate.dense.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.output.dense.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.0.output.dense.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.layernorm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.layernorm_before.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table - torch.Size([169, 8]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.self.query.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.self.query.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.self.key.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.self.key.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.self.value.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.self.value.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.output.dense.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.attention.output.dense.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.layernorm_after.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.layernorm_after.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.intermediate.dense.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.intermediate.dense.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.output.dense.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.blocks.1.output.dense.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.downsample.reduction.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.downsample.norm.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.1.downsample.norm.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.0.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.1.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.2.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.3.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.4.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.5.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.6.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.7.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.8.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.9.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.10.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.11.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.12.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.13.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.14.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.15.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.16.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.layernorm_before.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.layernorm_before.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table - torch.Size([169, 16]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.self.query.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.self.query.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.self.key.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.self.key.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.self.value.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.self.value.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.output.dense.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.attention.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.layernorm_after.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.layernorm_after.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.intermediate.dense.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.intermediate.dense.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.output.dense.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.blocks.17.output.dense.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.downsample.reduction.weight - torch.Size([1024, 2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.downsample.norm.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.2.downsample.norm.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.layernorm_before.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.layernorm_before.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table - torch.Size([169, 32]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.self.query.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.self.query.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.self.key.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.self.key.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.self.value.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.self.value.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.output.dense.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.attention.output.dense.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.layernorm_after.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.layernorm_after.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.intermediate.dense.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.intermediate.dense.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.output.dense.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.0.output.dense.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.layernorm_before.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.layernorm_before.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table - torch.Size([169, 32]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.self.query.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.self.query.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.self.key.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.self.key.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.self.value.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.self.value.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.output.dense.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.attention.output.dense.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.layernorm_after.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.layernorm_after.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.intermediate.dense.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.intermediate.dense.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.output.dense.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.encoder.layers.3.blocks.1.output.dense.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.layernorm.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.vision_model.layernorm.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.out_proj.0.0.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.out_proj.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.out_proj.0.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

backbone.out_proj.0.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_global_att_proj.0.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_global_att_proj.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_global_att_proj.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_global_att_proj.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

img_att_proj.0.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

img_att_proj.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

img_att_proj.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

img_att_proj.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.channel_attention.0.weight - torch.Size([80, 1280]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.channel_attention.0.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.channel_attention.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.channel_attention.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.channel_attention.3.weight - torch.Size([1280, 80]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.channel_attention.3.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.final_mapping.0.weight - torch.Size([768, 1280]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.final_mapping.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.final_mapping.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.final_mapping.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.ln_fused.weight - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.ln_fused.bias - torch.Size([1280]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.bn_camera.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_map.bn_camera.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

visual_feat_map.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

visual_feat_map.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_feat_map.0.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_feat_map.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_feat_map.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

text_feat_map.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

pos_embedding.position_embedding_head.0.weight - torch.Size([768, 3, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

pos_embedding.position_embedding_head.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

pos_embedding.position_embedding_head.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

pos_embedding.position_embedding_head.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

pos_embedding.position_embedding_head.3.weight - torch.Size([768, 768, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

pos_embedding.position_embedding_head.3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder_visual_pre_norm.0.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder_visual_pre_norm.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

full_visual_feat_map.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

full_visual_feat_map.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

full_pos_embedding.position_embedding_head.0.weight - torch.Size([768, 3, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

full_pos_embedding.position_embedding_head.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

full_pos_embedding.position_embedding_head.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

full_pos_embedding.position_embedding_head.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

full_pos_embedding.position_embedding_head.3.weight - torch.Size([768, 768, 1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

full_pos_embedding.position_embedding_head.3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder_full_visual_pre_norm.0.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

fusion_encoder_full_visual_pre_norm.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_bbox_head.clf_head.0.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_bbox_head.clf_head.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_bbox_head.clf_head.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_bbox_head.clf_head.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_bbox_head.clf_head.4.weight - torch.Size([1, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_bbox_head.clf_head.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_cls_head.clf_head.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_cls_head.clf_head.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_cls_head.clf_head.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_cls_head.clf_head.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_cls_head.clf_head.4.weight - torch.Size([18, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

target_cls_head.clf_head.4.bias - torch.Size([18]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.mlp.fc.linear.0.weight - torch.Size([384, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.mlp.fc.linear.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.mlp.fc.linear.1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.mlp.fc.linear.1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.mlp.linear.weight - torch.Size([1, 384]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.mlp.linear.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.linear_merge.0.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.linear_merge.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.linear_merge.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_visual.linear_merge.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.mlp.fc.linear.0.weight - torch.Size([384, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.mlp.fc.linear.0.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.mlp.fc.linear.1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.mlp.fc.linear.1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.mlp.linear.weight - torch.Size([1, 384]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.mlp.linear.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.linear_merge.0.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.linear_merge.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.linear_merge.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.attflat_lang.linear_merge.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.clf_head.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.clf_head.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.clf_head.1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.clf_head.1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.clf_head.4.weight - torch.Size([8864, 768]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  

qa_head.clf_head.4.bias - torch.Size([8864]): 
The value is the same before and after calling `init_weights` of MultiViewVLMBase3DQA  
